{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10729736,"sourceType":"datasetVersion","datasetId":6652188}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Requirement Installation","metadata":{}},{"cell_type":"code","source":"!pip install transformers[torch] datasets -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-02-13T13:09:22.625579Z","iopub.execute_input":"2025-02-13T13:09:22.625803Z","iopub.status.idle":"2025-02-13T13:09:32.756371Z","shell.execute_reply.started":"2025-02-13T13:09:22.625775Z","shell.execute_reply":"2025-02-13T13:09:32.755477Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import BlipProcessor, BlipForQuestionAnswering,BlipImageProcessor, AutoProcessor\nfrom transformers import BlipConfig\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:09:32.759158Z","iopub.execute_input":"2025-02-13T13:09:32.759759Z","iopub.status.idle":"2025-02-13T13:09:47.895242Z","shell.execute_reply.started":"2025-02-13T13:09:32.759720Z","shell.execute_reply":"2025-02-13T13:09:47.894247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:09:47.896455Z","iopub.execute_input":"2025-02-13T13:09:47.897047Z","iopub.status.idle":"2025-02-13T13:09:47.946476Z","shell.execute_reply.started":"2025-02-13T13:09:47.897011Z","shell.execute_reply":"2025-02-13T13:09:47.945578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_from_disk(\"/kaggle/input/medical-vqa-dataset/VQA_Medical_Dataset\")\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:09:47.947783Z","iopub.execute_input":"2025-02-13T13:09:47.948069Z","iopub.status.idle":"2025-02-13T13:09:49.773874Z","shell.execute_reply.started":"2025-02-13T13:09:47.948036Z","shell.execute_reply":"2025-02-13T13:09:49.772995Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sample Visualization","metadata":{}},{"cell_type":"code","source":"sample = dataset['train'][1]\nPIL_image = Image.fromarray(np.array(sample['image'])).convert('RGB')\nplt.imshow(sample['image'].convert('RGB'))\nprint(\"Question: {}\".format(sample['question']))\nprint(\"Answer: {}\".format(sample['answer']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:09:49.774791Z","iopub.execute_input":"2025-02-13T13:09:49.775042Z","iopub.status.idle":"2025-02-13T13:09:50.255807Z","shell.execute_reply.started":"2025-02-13T13:09:49.775020Z","shell.execute_reply":"2025-02-13T13:09:50.254872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BlipConfig\n\ntry:\n    # Attempt to load the config without local files\n    config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\")\nexcept Exception as e:\n    print(f\"Error occurred: {e}. Falling back to local files only.\")\n    # Fallback to loading the config with local files only\n    config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\", local_files_only=True)\n\n# Continue with the rest of your code\nprint(\"Config loaded successfully:\", config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:09:50.257630Z","iopub.execute_input":"2025-02-13T13:09:50.257917Z","iopub.status.idle":"2025-02-13T13:09:50.486216Z","shell.execute_reply.started":"2025-02-13T13:09:50.257893Z","shell.execute_reply":"2025-02-13T13:09:50.485467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sample = dataset['train'] \nval_sample = dataset['validation']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:16:18.118403Z","iopub.execute_input":"2025-02-13T13:16:18.118750Z","iopub.status.idle":"2025-02-13T13:16:18.131034Z","shell.execute_reply.started":"2025-02-13T13:16:18.118721Z","shell.execute_reply":"2025-02-13T13:16:18.130094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Data-loader","metadata":{}},{"cell_type":"code","source":"class VQADataset(torch.utils.data.Dataset):\n    def __init__(self, data, segment, text_processor, image_processor):\n        self.data = data\n        self.questions = data['question']\n        self.answers = data['answer']\n        self.text_processor = text_processor\n        self.image_processor = image_processor\n        self.max_length = 32\n        self.image_height = 128\n        self.image_width = 128\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # get image + text\n        answers = self.answers[idx]\n        questions = self.questions[idx]\n        image = self.data[idx]['image'].convert('RGB')\n        text = self.questions[idx]\n\n        image_encoding = self.image_processor(image,\n                                  do_resize=True,\n                                  size=(self.image_height,self.image_width),\n                                  return_tensors=\"pt\")\n\n        encoding = self.text_processor(\n                                  None,\n                                  text,\n                                  padding=\"max_length\",\n                                  truncation=True,\n                                  max_length = self.max_length,\n                                  return_tensors=\"pt\"\n                                  )\n        # remove batch dimension\n        for k,v in encoding.items():\n            encoding[k] = v.squeeze()\n        encoding[\"pixel_values\"] = image_encoding[\"pixel_values\"][0]\n        \n        # add labels\n        labels = self.text_processor.tokenizer.encode(\n            answers,\n            max_length= self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors='pt'\n        )[0]\n        encoding[\"labels\"] = labels\n\n        return encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:16:24.861739Z","iopub.execute_input":"2025-02-13T13:16:24.862067Z","iopub.status.idle":"2025-02-13T13:16:24.869518Z","shell.execute_reply.started":"2025-02-13T13:16:24.862040Z","shell.execute_reply":"2025-02-13T13:16:24.868656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipImageProcessor\n\n# Load text processor with error handling\ntry:\n    text_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n    print(\"Text processor loaded successfully from the Hugging Face model hub.\")\nexcept Exception as e:\n    print(f\"Error loading text processor: {e}. Falling back to local files only.\")\n    text_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", local_files_only=True)\n\n# Load image processor with error handling\ntry:\n    image_processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n    print(\"Image processor loaded successfully from the Hugging Face model hub.\")\nexcept Exception as e:\n    print(f\"Error loading image processor: {e}. Falling back to local files only.\")\n    image_processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", local_files_only=True)\n\n# Print confirmation\nprint(\"Text processor and image processor loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:16:29.387937Z","iopub.execute_input":"2025-02-13T13:16:29.388589Z","iopub.status.idle":"2025-02-13T13:16:30.403890Z","shell.execute_reply.started":"2025-02-13T13:16:29.388558Z","shell.execute_reply":"2025-02-13T13:16:30.403070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_vqa_dataset = VQADataset(data=train_sample,\n                     segment='train',\n                     text_processor = text_processor,\n                     image_processor = image_processor\n                         )\n\nval_vqa_dataset = VQADataset(data=val_sample,\n                     segment='validation',\n                     text_processor = text_processor,\n                     image_processor = image_processor\n                         )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:17:14.840044Z","iopub.execute_input":"2025-02-13T13:17:14.840870Z","iopub.status.idle":"2025-02-13T13:17:14.858891Z","shell.execute_reply.started":"2025-02-13T13:17:14.840840Z","shell.execute_reply":"2025-02-13T13:17:14.858295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_vqa_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:17:20.598808Z","iopub.execute_input":"2025-02-13T13:17:20.599161Z","iopub.status.idle":"2025-02-13T13:17:20.829319Z","shell.execute_reply.started":"2025-02-13T13:17:20.599123Z","shell.execute_reply":"2025-02-13T13:17:20.828465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = [item['input_ids'] for item in batch]\n    pixel_values = [item['pixel_values'] for item in batch]\n    attention_mask = [item['attention_mask'] for item in batch]\n    labels = [item['labels'] for item in batch]\n    # create new batch\n    batch = {}\n    batch['input_ids'] = torch.stack(input_ids)\n    batch['attention_mask'] = torch.stack(attention_mask)\n    batch['pixel_values'] = torch.stack(pixel_values)\n    batch['labels'] = torch.stack(labels)\n\n    return batch\n\ntrain_dataloader = DataLoader(train_vqa_dataset,\n                              collate_fn=collate_fn,\n                              batch_size=32,\n                              shuffle=False)\nval_dataloader = DataLoader(val_vqa_dataset,\n                            collate_fn=collate_fn,\n                            batch_size=32,\n                            shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:17:24.866650Z","iopub.execute_input":"2025-02-13T13:17:24.866992Z","iopub.status.idle":"2025-02-13T13:17:24.873528Z","shell.execute_reply.started":"2025-02-13T13:17:24.866962Z","shell.execute_reply":"2025-02-13T13:17:24.872528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nfor k,v in batch.items():\n    print(k, v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:17:33.846432Z","iopub.execute_input":"2025-02-13T13:17:33.847070Z","iopub.status.idle":"2025-02-13T13:17:34.571410Z","shell.execute_reply.started":"2025-02-13T13:17:33.847038Z","shell.execute_reply":"2025-02-13T13:17:34.570501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Model","metadata":{}},{"cell_type":"code","source":"try:\n    # Attempt to load the model without local files\n    model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n    print(\"Model loaded successfully from the Hugging Face model hub.\")\nexcept Exception as e:\n    print(f\"Error occurred: {e}. Falling back to local files only.\")\n    # Fallback to loading the model with local files only\n    model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", local_files_only=True)\n\n# Move the model to the specified device (GPU or CPU)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:17:37.591746Z","iopub.execute_input":"2025-02-13T13:17:37.592525Z","iopub.status.idle":"2025-02-13T13:17:49.148063Z","shell.execute_reply.started":"2025-02-13T13:17:37.592494Z","shell.execute_reply":"2025-02-13T13:17:49.147191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nimage_mean = image_processor.image_mean\nimage_std = image_processor.image_std","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:18:07.411138Z","iopub.execute_input":"2025-02-13T13:18:07.411495Z","iopub.status.idle":"2025-02-13T13:18:07.418794Z","shell.execute_reply.started":"2025-02-13T13:18:07.411456Z","shell.execute_reply":"2025-02-13T13:18:07.417808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_idx = 1\n\nunnormalized_image = (batch[\"pixel_values\"][batch_idx].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n\nprint(\"Question: \",text_processor.decode(batch[\"input_ids\"][batch_idx]))\nprint(\"Answer: \",text_processor.decode(batch[\"labels\"][batch_idx]))\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:18:09.976114Z","iopub.execute_input":"2025-02-13T13:18:09.976880Z","iopub.status.idle":"2025-02-13T13:18:10.297514Z","shell.execute_reply.started":"2025-02-13T13:18:09.976846Z","shell.execute_reply":"2025-02-13T13:18:10.296773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import StepLR\nfrom nltk.translate.bleu_score import sentence_bleu\nimport nltk\n\nnltk.download('punkt')  # Ensure BLEU tokenizer is available\n\n# Hyperparameters\nepochs = 50\naccumulation_steps = 4  # Gradients will be accumulated over 4 mini-batches\nmax_grad_norm = 1.0\nbest_bleu = 0.0  # Track best BLEU score\n\n# Learning Rate Scheduler\nscheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n\n# Mixed Precision Training\nscaler = GradScaler()\n\n# Start training\nfor epoch in range(epochs):\n    print(f\"Epoch: {epoch}\")\n    model.train()\n    \n    total_loss = []\n    for i, batch in enumerate(tqdm(train_dataloader, disable=False)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n\n        with autocast():  \n            outputs = model(**batch)\n            loss = outputs.loss\n        \n        # Gradient Accumulation\n        scaler.scale(loss).backward()\n\n        if (i + 1) % accumulation_steps == 0:\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            \n            # Update optimizer step\n            scaler.step(optimizer)\n            scaler.update()\n\n            optimizer.zero_grad()\n\n        total_loss.append(loss.item())\n\n    # Learning Rate Scheduling\n    scheduler.step()\n\n    # Validation with BLEU score\n    model.eval()\n    val_loss = 0\n    total_bleu = 0\n    count = 0\n    \n    with torch.no_grad():\n        for batch in val_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            with autocast():\n                # Compute loss separately\n                outputs = model(**batch)\n                val_loss += outputs.loss.item()\n                \n                # Generate predictions\n                generated_outputs = model.generate(\n                    pixel_values=batch['pixel_values'], \n                    input_ids=batch['input_ids'],\n                    max_new_tokens=50  # Explicitly set max_new_tokens\n                )\n    \n            # Decode predictions and references\n            predicted_answer = text_processor.decode(generated_outputs[0], skip_special_tokens=True)\n            reference_answer = text_processor.decode(batch['labels'][0], skip_special_tokens=True)\n    \n            # Compute BLEU score\n            reference_tokens = [reference_answer.split()]\n            predicted_tokens = predicted_answer.split()\n            bleu_score = sentence_bleu(reference_tokens, predicted_tokens)\n            total_bleu += bleu_score\n            count += 1\n\n\n\n    val_loss /= len(val_dataloader)\n    avg_bleu = total_bleu / count  # Compute average BLEU score\n\n    print(f\"Epoch {epoch} - Training Loss: {sum(total_loss)}, Validation Loss: {val_loss}, BLEU Score: {avg_bleu:.4f}\")\n\n    # Save best model based on BLEU score\n    if avg_bleu > best_bleu:\n        best_bleu = avg_bleu\n        fine_tuned_model_path = \"/kaggle/working/medical_vqa_blip\"\n\n        # Save fine-tuned model\n        model.save_pretrained(\"/kaggle/working/medical_vqa_blip\")\n\n        # Save tokenizer and image processor\n        text_processor.save_pretrained(\"/kaggle/working/medical_vqa_blip/text_processor\")\n        image_processor.save_pretrained(\"/kaggle/working/medical_vqa_blip/image_processor\")\n        print(f\"Fine-tuned model and processor saved to {fine_tuned_model_path} with BLEU Score: {best_bleu:.4f}\")\n\nprint(\"Training complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:20:35.654161Z","iopub.execute_input":"2025-02-13T13:20:35.654467Z","iopub.status.idle":"2025-02-13T13:22:09.367516Z","shell.execute_reply.started":"2025-02-13T13:20:35.654444Z","shell.execute_reply":"2025-02-13T13:22:09.366128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Assuming `image_mean` and `image_std` are defined as:\nimage_mean = [0.48145466, 0.4578275, 0.40821073]  # Standard mean values\nimage_std = [0.26862954, 0.26130258, 0.27577711]  # Standard deviation values\n\n# Loop over validation dataset\nfor x in range(50):\n    sample = val_vqa_dataset[x]\n\n    # Decode question\n    question_text = text_processor.decode(sample['input_ids'], skip_special_tokens=True)\n\n    # Convert sample to batch format and move to GPU\n    sample = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n\n    # Generate prediction\n    with torch.no_grad():\n        outputs = model.generate(pixel_values=sample['pixel_values'], input_ids=sample['input_ids'])\n\n    # Decode predicted and actual answers\n    predicted_answer = text_processor.decode(outputs[0], skip_special_tokens=True)\n    actual_answer = text_processor.decode(sample['labels'][0], skip_special_tokens=True) if 'labels' in sample else \"N/A\"\n\n    # Unnormalize image\n    unnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\n    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)  # Convert from (C, H, W) to (H, W, C)\n\n    # Display the image with Matplotlib\n    plt.figure(figsize=(6, 6))\n    plt.imshow(unnormalized_image)\n    plt.axis(\"off\")\n    plt.title(f\"Question: {question_text}\\nTrue Answer: {actual_answer}\\nPredicted: {predicted_answer}\")\n    plt.show()\n\n    print(\"###################################################################\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T13:09:51.130110Z","iopub.status.idle":"2025-02-13T13:09:51.130521Z","shell.execute_reply.started":"2025-02-13T13:09:51.130310Z","shell.execute_reply":"2025-02-13T13:09:51.130331Z"}},"outputs":[],"execution_count":null}]}