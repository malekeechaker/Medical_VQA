{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10729736,"sourceType":"datasetVersion","datasetId":6652188}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nA successfully trained and inferred medical Visual Question Answering (VQA) model represents a significant advancement in the field of healthcare and medical image analysis. Such a model combines computer vision and natural language processing to provide valuable insights and answers to medical professionals, researchers, and patients.\n\n![intro](https://imageio.forbes.com/specials-images/imageserve/636063ae49e46108de0472a1/Medical-technology-concept--Remote-medicine--Electronic-medical-record-/960x0.jpg)\n","metadata":{}},{"cell_type":"markdown","source":"\nHere are some key conclusions and takeaways from the development and deployment of a successful medical VQA model:\n\n* **Improved Clinical Decision Support**: A well-trained medical VQA model enhances clinical decision-making by allowing healthcare providers to ask questions about medical images (e.g., X-rays, MRIs, CT scans) and receive accurate, rapid answers. This can lead to faster diagnoses and treatment plans.\n\n* **Reducing Interpretation Errors**: Human interpretation of medical images can be subjective and prone to errors. A VQA model can provide objective, consistent, and evidence-based interpretations, helping to reduce diagnostic inaccuracies.\n\n* **Time Efficiency**: The model's ability to quickly analyze images and answer questions can save valuable time for healthcare professionals, leading to more efficient patient care.\n\n* **Accessibility**: Patients and non-specialist healthcare providers can benefit from a medical VQA system by obtaining easy-to-understand information about their health conditions, potentially improving health literacy.\n\n* **Learning and Training Aid**: Medical VQA models can serve as educational tools for medical students, residents, and even experienced practitioners. They can be used to explain complex medical concepts and imaging findings.\n\n* **Research Assistance**: Researchers can leverage the model to analyze large datasets of medical images more effectively. It can assist in extracting meaningful insights from these datasets, potentially leading to new discoveries in medical science.\n\n* **Cross-Specialty Applicability**: A well-designed medical VQA model can be adapted to various medical specialties, from radiology and pathology to cardiology and dermatology. This versatility makes it a valuable asset across different healthcare domains.\n\n* **Ethical Considerations**: It's essential to address ethical concerns related to privacy, security, and bias when deploying medical VQA models in healthcare settings. Ensuring patient data protection and model fairness is critical.\n\n* **Continuous Improvement**: Model performance and accuracy should be continuously monitored and improved over time. Regular updates and retraining are necessary to keep up with evolving medical knowledge and technologies.\n\n* **Collaboration**: Successful implementation of medical VQA models often requires collaboration between machine learning experts, healthcare professionals, and ethicists to ensure that the technology is used responsibly and effectively.","metadata":{}},{"cell_type":"markdown","source":"## BLIP\n\n## Overview\nThe BLIP model was proposed in BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\n\nBLIP is a model that is able to perform various multi-modal tasks including\n\n* Visual Question Answering\n* Image-Text retrieval (Image-text matching)\n* Image Captioning\n\n\nThe abstract from the paper is the following:\n\n`Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.\n`\n\n![blip](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif)\n\nSource - [HuggingFace](https://huggingface.co/)","metadata":{}},{"cell_type":"markdown","source":"## Visual Question Answering\n\n**What is visual Question Answering?**\n\nVisual Question Answering (VQA) is a task in computer vision that involves answering questions about an image. The goal of VQA is to teach machines to understand the content of an image and answer questions about it in natural language.\n\n![vqa](https://visualqa.org/static/img/yinyang.png)","metadata":{}},{"cell_type":"markdown","source":"## Requirement Installation","metadata":{}},{"cell_type":"code","source":"!pip install transformers[torch] datasets -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-02-12T10:22:31.382325Z","iopub.execute_input":"2025-02-12T10:22:31.382527Z","iopub.status.idle":"2025-02-12T10:23:06.271191Z","shell.execute_reply.started":"2025-02-12T10:22:31.382500Z","shell.execute_reply":"2025-02-12T10:23:06.270057Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import BlipProcessor, BlipForQuestionAnswering,BlipImageProcessor, AutoProcessor\nfrom transformers import BlipConfig\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T10:27:05.426854Z","iopub.execute_input":"2025-02-12T10:27:05.427213Z","execution_failed":"2025-02-12T10:27:13.420Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_from_disk(\"/kaggle/input/medical-vqa-dataset/VQA_Medical_Dataset\")\nprint(dataset)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sample Visualization","metadata":{}},{"cell_type":"code","source":"sample = dataset['train'][1]\nPIL_image = Image.fromarray(np.array(sample['image'])).convert('RGB')\nplt.imshow(sample['image'].convert('RGB'))\nprint(\"Question: {}\".format(sample['question']))\nprint(\"Answer: {}\".format(sample['answer']))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\", local_files_only=True)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = dataset['train']\nval_data = dataset['validation']","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Data-loader","metadata":{}},{"cell_type":"code","source":"class VQADataset(torch.utils.data.Dataset):\n    def __init__(self, data, segment, text_processor, image_processor):\n        self.data = data\n        self.questions = data['question']\n        self.answers = data['answer']\n        self.text_processor = text_processor\n        self.image_processor = image_processor\n        self.max_length = 32\n        self.image_height = 128\n        self.image_width = 128\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # get image + text\n        answers = self.answers[idx]\n        questions = self.questions[idx]\n        image = self.data[idx]['image'].convert('RGB')\n        text = self.questions[idx]\n\n        image_encoding = self.image_processor(image,\n                                  do_resize=True,\n                                  size=(self.image_height,self.image_width),\n                                  return_tensors=\"pt\")\n\n        encoding = self.text_processor(\n                                  None,\n                                  text,\n                                  padding=\"max_length\",\n                                  truncation=True,\n                                  max_length = self.max_length,\n                                  return_tensors=\"pt\"\n                                  )\n        # remove batch dimension\n        for k,v in encoding.items():\n            encoding[k] = v.squeeze()\n        encoding[\"pixel_values\"] = image_encoding[\"pixel_values\"][0]\n        \n        # add labels\n        labels = self.text_processor.tokenizer.encode(\n            answers,\n            max_length= self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors='pt'\n        )[0]\n        encoding[\"labels\"] = labels\n\n        return encoding","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", local_files_only=True)\nimage_processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", local_files_only=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_vqa_dataset = VQADataset(data=train_data,\n                     segment='train',\n                     text_processor = text_processor,\n                     image_processor = image_processor\n                         )\n\nval_vqa_dataset = VQADataset(data=train_data,\n                     segment='validation',\n                     text_processor = text_processor,\n                     image_processor = image_processor\n                         )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_vqa_dataset[0]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = [item['input_ids'] for item in batch]\n    pixel_values = [item['pixel_values'] for item in batch]\n    attention_mask = [item['attention_mask'] for item in batch]\n    labels = [item['labels'] for item in batch]\n    # create new batch\n    batch = {}\n    batch['input_ids'] = torch.stack(input_ids)\n    batch['attention_mask'] = torch.stack(attention_mask)\n    batch['pixel_values'] = torch.stack(pixel_values)\n    batch['labels'] = torch.stack(labels)\n\n    return batch\n\ntrain_dataloader = DataLoader(train_vqa_dataset,\n                              collate_fn=collate_fn,\n                              batch_size=32,\n                              shuffle=False)\nval_dataloader = DataLoader(val_vqa_dataset,\n                            collate_fn=collate_fn,\n                            batch_size=32,\n                            shuffle=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nfor k,v in batch.items():\n    print(k, v.shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build Model","metadata":{}},{"cell_type":"code","source":"model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", local_files_only=True )\nmodel.to(device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nimage_mean = image_processor.image_mean\nimage_std = image_processor.image_std","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_idx = 1\n\nunnormalized_image = (batch[\"pixel_values\"][batch_idx].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n\nprint(\"Question: \",text_processor.decode(batch[\"input_ids\"][batch_idx]))\nprint(\"Answer: \",text_processor.decode(batch[\"labels\"][batch_idx]))\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel.train()\nfor epoch in range(10):\n    print(f\"Epoch: {epoch}\")\n    total_loss = []\n    for batch in tqdm(train_dataloader, disable=True):  # Disabling tqdm\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss.append(loss.item())\n        loss.backward()\n        optimizer.step()\n    \n    print(\"Loss:\", sum(total_loss))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# add batch dimension + move to GPU|\nfor x in range(100):\n    sample = val_vqa_dataset[x]\n    print(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\n    sample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n    # forward pass\n    outputs = model.generate(pixel_values=sample['pixel_values'],\n                            input_ids=sample['input_ids'])\n    print(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\n    print(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n    #########################################################################\n    unnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\n    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n    display(Image.fromarray(unnormalized_image))\n    #########################################################################\n    print(\"###################################################################\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for x in range(500,600):\n    sample = val_vqa_dataset[x]\n    print(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\n    sample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n    # forward pass\n    outputs = model.generate(pixel_values=sample['pixel_values'],\n                            input_ids=sample['input_ids'])\n    print(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\n    print(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n    #########################################################################\n    unnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\n    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n    display(Image.fromarray(unnormalized_image))\n    #########################################################################\n    print(\"###################################################################\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 751\nsample = val_vqa_dataset[idx]\nprint(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\nsample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n# forward pass\noutputs = model.generate(pixel_values=sample['pixel_values'],\n                        input_ids=sample['input_ids'])\nprint(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\nprint(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n#########################################################################\nunnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 700\nsample = val_vqa_dataset[idx]\nprint(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\nsample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n# forward pass\noutputs = model.generate(pixel_values=sample['pixel_values'],\n                        input_ids=sample['input_ids'])\nprint(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\nprint(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n#########################################################################\nunnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 720\nsample = val_vqa_dataset[idx]\nprint(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\nsample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n# forward pass\noutputs = model.generate(pixel_values=sample['pixel_values'],\n                        input_ids=sample['input_ids'])\nprint(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\nprint(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n#########################################################################\nunnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 790\nsample = val_vqa_dataset[idx]\nprint(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\nsample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n# forward pass\noutputs = model.generate(pixel_values=sample['pixel_values'],\n                        input_ids=sample['input_ids'])\nprint(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\nprint(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n#########################################################################\nunnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 885\nsample = val_vqa_dataset[idx]\nprint(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\nsample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n# forward pass\noutputs = model.generate(pixel_values=sample['pixel_values'],\n                        input_ids=sample['input_ids'])\nprint(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\nprint(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n#########################################################################\nunnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 822\nsample = val_vqa_dataset[idx]\nprint(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\nsample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n# forward pass\noutputs = model.generate(pixel_values=sample['pixel_values'],\n                        input_ids=sample['input_ids'])\nprint(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\nprint(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n#########################################################################\nunnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 770\nsample = val_vqa_dataset[idx]\nprint(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\nsample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n# forward pass\noutputs = model.generate(pixel_values=sample['pixel_values'],\n                        input_ids=sample['input_ids'])\nprint(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\nprint(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n#########################################################################\nunnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 777\nsample = val_vqa_dataset[idx]\nprint(\"Question: \",text_processor.decode(sample['input_ids'], skip_special_tokens=True))\nsample = {k: v.unsqueeze(0).to(device) for k,v in sample.items()}\n\n# forward pass\noutputs = model.generate(pixel_values=sample['pixel_values'],\n                        input_ids=sample['input_ids'])\nprint(\"Predicted Answer: \",text_processor.decode(outputs[0],skip_special_tokens=True))\nprint(\"Actual Answer: \",text_processor.decode(sample['labels'][0], skip_special_tokens=True))\n#########################################################################\nunnormalized_image = (sample[\"pixel_values\"][0].cpu().numpy() * np.array(image_std)[:, None, None]) + np.array(image_mean)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nplt.imshow(Image.fromarray(unnormalized_image))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T10:24:58.691Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nA successfully trained and inferred medical Visual Question Answering model has the potential to revolutionize healthcare by enhancing diagnostic accuracy, improving efficiency, and expanding access to medical information. However, careful consideration of ethical, privacy, and regulatory issues is crucial to ensure its safe and responsible use in clinical practice.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}